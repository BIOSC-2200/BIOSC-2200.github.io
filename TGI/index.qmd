---
title: "Transmission of Genetic Information"
engine: knitr
format: 
  html: 
    toc: true
    toc-depth: 3
    toc-title: Contents
    self-contained: true
    css: ../custom.css
    link-external-newwindow: true
webr: 
  packages: ["ggplot2", "dplyr", "forcats", "tibble"]
filters: 
  - webr
bibliography: ../Genetics_Discussion.bib
csl: ../evolution.csl
code-annotations: hover
---

```{r}
#| echo: false
#| message: false
#| warning: false

library(ggplot2)
library(tidyr)
library(dplyr)
library(gt)
library(grid)
library(patchwork)
library(magick)

ggplot2::theme_set(ggplot2::theme_light())
```

```{webr-r}
#| context: setup
#| echo: false
#| message: false
#| warning: false

library(ggplot2)
library(dplyr)
library(forcats)
library(tibble)

ggplot2::theme_set(ggplot2::theme_light())
```


## Introduction

### Contact information

If you have questions about these exercises, please contact Dr. Kevin Middleton (middletonk@missouri.edu) or drop by Tucker 222.


### Motivation and Goals

Many topics within the study of genetics have a strong link to numerical or quantitative analysis. Long before the identification of DNA as the primary carrier of genetic information, both scientists and non-scientists were conducting breeding experiments. Initially these "experiments" had the goal of domesticating animals for human benefits (e.g., cattle, pigs, goats, dogs). Although not true experiments as we know them today, the end result of those efforts played a large part in the rise of agriculture and civilizations.

At least ten thousand years passed before the investigations of predictable breeding phenotypes in pea plants by Gregor Mendel (1822-1884), which ultimately led to modern genetics. The study of genetics during the first half of the 20th century was carried out without an understanding of the structure of DNA. Even without this specific knowledge, scientists were able to make great strides in understanding the patterns of evolution in populations. Much of this research was aided by simultaneous advances in statistics during the same time period, which in many cases were being developed to help make sense of the vast array of data that was being collected. 

We have developed these exercises to give you an introduction to some of the statistical approaches that are used in the study of genetics. We hope that you will use these to further develop your understanding of topics in genetics that you are covering in lecture and that you will begin to gain some intuition about the methods used to test hypotheses. Some of you might even wish to go deeper into understanding the math and computer code behind these exercises.

Because quantitative aspects of genetics necessarily involve numbers and equations, we will need to use them. By working through examples, we hope to demystify the numbers. Complex math is not usually necessary, and most can be accomplished with only addition, subtraction, multiplication, and division.

Finally, we will use computer code to carry out analyses and generate plots. These exercises are not designed to be a course in coding, and you do not need to learn any additional material to use the exercises. You will be able to work through by just clicking the "Run Code" button, entering or changing a few values, and clicking "Run Code" again. As mentioned above, if you are interested in the computational side, we encourage you to edit, explore, and learn all you can.^[We are happy to provide additional resources. Just ask.] But that is not necessary to use these exercises for your benefit.


### How to use these exercises

This web page is refreshed each time you visit. It is available any time, both on and off campus. It should look the same on laptops, tablets, and phones, although the text will be necessarily small on your phone.

The first and most important directive is to read carefully, because there is no need to rush through. The exercises are designed to be worked linearly. Each piece builds on the those that come before it. Work through deliberately, when you get to a code chunk (see below), pay attention to any edits or data that you need to input.

There are prompts throughout in that ask you to think, make a prediction, evaluate your prediction, etc. These prompts look like the following: a box with green text followed by a box below it in which you can record your answer.

> Take a few moments to think about what you hope to gain from these exercises.

<textarea rows="4" cols="75">
Type your response here.
</textarea>


#### Save your work

Unfortunately, there is no way to save your work for later. And *definitely do not reload this web page*, because you will lose your work (a downside of the page being refreshed each time you visit it). 

To save your work there are a few options:

- Open the associated Word file (link in the table of contents) and paste your responses into that document.
- Print this page to a file to save your answers.
    - If you are on a laptop, if you choose to print from the browser, there will be an option to save the page to PDF.
    - On tablets and phones, use the "share" button. For example, in iOS choosing "Options" in the share menu will allow you to send the page as a PDF.

::: {.callout-note}

Reminder: Save your work regularly.

:::


#### The R Programming Language

These exercises use the statistical programming language [R](https://r-project.org/) for analysis and making plots. You do not need to already know how to code in R or any other language to use these exercises. Everything you need to know will be provided.

On this web page, R is running within your web browser^[We are using the [webR framework](https://docs.r-wasm.org/webr/latest/).], which means that you do not need to install any software (nor is any software permanently installed on your device). When you first load the web page, it will take a few seconds for the software to load. When the loading step is complete, you will see the icon

![](../images/webr.png){fig-align="center"}

at the top of the page. Once you see the green "Ready!" sign, everything is set.


#### Code blocks

As you work through, you will encounter code blocks. Some code blocks will run when the page is first loaded (no interaction required on your part). Other code blocks won't execute until you ask.

These code blocks will look like this, with a "Run Code" button:

```{webr-r}
2 + 2
```

Clicking the "Run Code" button will cause R to run the code and print the result (in this case `4` -- not very interesting, but we have to start somewhere). The simplest example is to just use R as a fancy calculator. If you haven't already, click "Run Code" above.

Feel free to try different values and different operators (`+`, `-`, `*`, `/`, `^`). You can't break anything. The worst that will happen is that you will get a syntax error. Execute the code below to see what a syntax error looks like.

```{webr-r}
2 * a12
```

You can edit the block to make the code execute successfully, or just leave it as is and move on.


## Learning Objectives

The learning objectives for this exercise are:

- Calculate the probability of a particular gamete being produced from an individual, assuming independent assortment.
- Calculate the probability of a particular genotype in the offspring, given independent assortment and random fertilization between two individuals.
- Design genetic crosses to provide information about genes, alleles, and gene functions.
- Use a chi-squared test to determine how well data from a genetic cross fits theoretical predictions.
- Explain how sample size is related to the certainty of a chi-squared test.


## Review of Inheritance

When gametes are produced, each gamete receives a single copy of each gene. Because each of the F~1~ parents has two copies, there are four possible alleles that can be passed on (for example two copies of *D* and two copies of *d* in @fig-punnett). A Punnett square can be used to enumerate the possible genotype combinations that result from the proposed cross. By joining the genes from each parent, the resulting possible offspring genotypes can be determined. These genotypes can be converted into phenotypes if the nature of dominance of the alleles in known (@fig-punnett). In this example the *D* allele for the "tall" phenotype is dominant relative to the *d* allele.

![The process of gamete formation leads to predictable genotypic and phenotypic proportions in the F~2~ generation. Image modified from Klug et al. [-@Klug2019-cs]](../images/punnett.png){fig-alt="" #fig-punnett}

The 1:2:1 genotypic ratio and 3:1 phenotypic ratio in the heterozygote cross example above represent theoretical probabilities for the distributions of genotypes and phenotypes. These ratios can be tested statistically to determine if a sample deviates significantly from the expected proportions. The exercises that follow will allow you explore the statistics of genetics and give you some practice testing whether observations follow the predicted patterns. 


## Proportions and Probabilities

When thinking about genotypes and phenotypes, it can be useful to sometimes think of counts as *proportions* and sometimes as *probabilities.* The 1:2:1 genotypic ratio above can be thought of as 

1. *Proportions*: 1/4 *DD*, 2/4 *Dd*, and 1/4 *dd*
2. *Percentages*: 25% *DD*, 50% *Dd*, and 25% *dd*
3. *Probabilities*: 0.25 *DD*, 0.5 *Dd*, and 0.25 *dd*

All are equal and mean the same thing. In some contexts, using one vs. another makes more sense. This set of proportions and probabilities all sum to one and represents the full range of possible combinations.

Probabilities are very predictable in the long run but are often unpredictable in the short run. Consider only one gamete being produced, say a sperm. Each sperm that is produced from a heterozygote *Dd* can carry either the *D* or *d* allele.

We can simulate the production of a single sperm with the following code. First we create an object that holds the possible `allele`s: "D" and "d". The code `sample(allele, size = 1)` randomly samples 1 of the possible alleles. Run this code a few times to see the first few gametes produced.

```{webr-r}
allele <- c("D", "d")
sample(allele, size = 1)
```

You probably noticed that, quite often, sequences of two or more *D* or *d* were produced randomly.

The code block below repeats this sampling process 10 times and counts up the numbers of *D* and *d* gametes produced. The function `replicate()` generates the samples and `table()` counts *D*s and *d*s.

> How many *D* and *d* gametes do you predict from a total of 10? How confident are you in your answer?

<textarea rows="2" cols="75">
Enter your prediction here.
</textarea>

```{webr-r}
replicate(n = 10, sample(allele, size = 1)) |> 
  table()
```

Run the code a few times to generate new samples of 10 gametes. Revisit your prediction.

Now gradually increase the number of gametes produced by changing `n = 10` to `n = 50`, `n = 100`, etc. You can choose larger numbers, but avoid going over about 10,000 or it will run very slowly.

> What happens to the relative counts of *D* and *d* as `n` increases? What does this tell you about our ability to predict the exact counts either when sample sizes are small *or* when sample sizes are large? In general, how often do you see exactly 50% *D* and 50% *d*?

<textarea rows="3" cols="75">

</textarea>


## Blood types in humans

To explore the concepts of independent assortment and probabilities, we will use the blood-typing system that is used to classify human blood types based on the expression of different antigens on the surface of red blood cells (@fig-RBC). Blood types are important for transfusion medicine, where the mixing of incompatible blood types can have fatal results.

![Three of the components of blood are white blood cells (involved in the immune response), platelets (involved in blood clotting), and red blood cells, which primarily carry oxygen. Image from [learn.genetics](https://learn.genetics.utah.edu/)](https://learn-genetics.b-cdn.net/basics/blood/images/complexblood.png){width=100% fig-align="left" fig-alt="" #fig-RBC}

Blood typing in humans primarily uses the ABO system, although almost 30 other blood typing systems exist, which focus on other aspects of red blood cell structure and physiology. 

A and/or B refer to the presence of A and/or B antigens of the surface of the red blood cells. O denotes the absence of both A and B antigens. The [American Red Cross](https://www.redcrossblood.org/donate-blood/blood-types.html) has a website with animations about what blood types are compatible with other blood types. In general, A must be matched with A and B with B or a cross-reaction will take place. Because type O blood lacks both A and B antigens, it is considered a "universal donor". And because type AB blood has both A and B antigens, it is considered a "universal recipient".

@fig-ABOgenes shows the Punnett square for human blood types. Each parent can provide alleles for A, B, or no antigens (O), leading to a complex 1:2:1:1:1:2:1 ratio.

![The possible gamete genotypes and combinations for human ABO alleles and associated blood types. Individuals don't have all of A, B and O -- this table illustrates the possibilities for individual gametes. Image from [learn.genetics](https://learn.genetics.utah.edu/)](https://learn-genetics.b-cdn.net/basics/blood/images/inherit.png){width=50% fig-align="center" fig-alt="" #fig-ABOgenes}

The right side of @fig-ABOgenes shows the blood types (i.e., phenotypes) associated with each of the possible genotypes. Although there are 7 possible genotypes, there are only 4 possible blood types: A, B, AB, and O.

Looking at the figure, you may have wondered (1) why there is no type AO or BO blood and (2) how type AB blood is produced. 

1. Because the O allele has no associated antigen, an individual with A+O alleles will have type A blood and an individual with B+O will have type B blood (the same is true for O+A and O+B). So we can say that A and B are **dominant** with respect to O.
2. Because the A and B alleles produce A and B antigens, respectively, an individual with A+B or B+A will have type AB blood. The A and B alleles are thus **co-dominant**.


### Rh Factor

Another important feature of red blood cells that is important for transplant medicine is the Rh factor. The genetics of Rh factor is much more complicated than the ABO system, with approximately 50 different proteins involved [@Dean2005-dm]. Because a few proteins are most commonly involved, this system can be summarized as *Rh+* and *Rh-* (Rh-positive and Rh-negative) without going into all the details. 

ABO type and Rh factor are determined by separate sets of genes, meaning that Rh factor is inherited separately from ABO type. Thus the rules of **Independent Assortment** apply. Each ABO blood type can be associated with Rh+ or Rh-.


### Probabilities of blood types in a population

The distributions of A, B, AB, and O blood types as well as Rh factors differs among populations. In a hypothetical population, blood type probabilities for all the combinations of ABO types and Rh status are summarized in the following table.

```{r}
#| echo: false

BT <- tribble(
  ~ABO_Type, ~Rh, ~Probability,
  "A", "-", 0.005,
  "A", "+", 0.27,
  "AB", "-", 0.001,
  "AB", "+", 0.07,
  "B", "-", 0.004,
  "B", "+", 0.25,
  "O", "-", 0.01,
  "O", "+", 0.39)

BT |> gt() |> 
  cols_label(ABO_Type = md("**ABO**"),
             Rh = md("**Rh Factor**"),
             Probability = md("**Probability**"))

```

Looking at tables of numbers is difficult, so we will plot the data. The relative proportions of the eight different blood types can be shown on a bar chart, where the height of the bar is proportional to the probability of an individual having a specific ABO/Rh combination: 

```{r}
#| echo: false

BT <- BT |> 
  unite(col = "Type", ABO_Type, Rh, sep = "")

ggplot(BT, aes(x = Type, y = Probability)) +
  geom_col()
```

Visualizing the table above in this way really emphasizes the relative rarity of Rh-negative blood types in this population.

Note that sum of all the probabilities is 1. When studying the range of probabilities for an event (here a person's blood type), it is important that all the probabilities add up to 1, which means that we have accounted for all the possible outcomes.

```{webr-r}
sum(c(0.005, 0.270, 0.001, 0.070, 0.004, 0.250, 0.010, 0.390))
```

`sum()` adds up the values passed to it. `c()` makes a collection of numbers, which here is the set of probabilities.

If a large number of people have their blood typed, the proportions of people with each blood type should be approximately equal to the overall proportions in the table and figure above.


### Combining probabilities

Because of independent assortment, each of the 8 possible ABO types + Rh factor are independent of each other, which is to say that no one person can have multiple blood types. We can use the rules of addition to determine the probability that a randomly selected individual has a certain blood type. The first few are provided for you. See if you can figure out the rest (here we are just using R as a calculator).

Type O blood

```{webr-r}
0.39 + 0.01
```

Type AB blood

```{webr-r}
0.07 + 0.001
```

Type A *or* type B blood with any Rh factor

```{webr-r}

```

Any ABO type with Rh+

```{webr-r}

```

*Not* Type AB blood

```{webr-r}

```

These are all examples of the *Addition Rule* for probabilities. For independent events, you can add the individual probabilities to get the overall probability for a set of events. Generally these take the form of A *or* B -- like rolling a 1 or a 2 from a single die roll. Each has a probability of 1/6, so the probability of 1 *or* 2 is 1/6 + 1/6 = 2/6 = 1/3.

For conditional events, the kinds of which result from independent assortment, we have to multiply two probabilities (the *Multiplication Rule* for probabilities). The flowchart below shows the calculation of the probabilities for O+ and O- blood types. 

Starting from the left, we have all blood types. In this population, 40% have type O blood (including both Rh+ and Rh-), and 60% have all the other types combined. 

In this example, we will only follow the type O blood paths. Among those with type O blood, 97.5% have the Rh+ phenotype, and the remaining 2.5% have the Rh- phenotype. Because ABO type and Rh factor assort independently, the separate probabilities are multiplied to get the final probabilities. $0.4 \times 0.025 = 0.01$ and $0.4 \times 0.975 = 0.39$. You can confirm that these match the probabilities in the table.

```{r}
#| echo: false

library(ggflowchart)

FC <- tribble(
  ~ from, ~ to,
  "All\nBlood", "Type O\n0.40",
  "All\nBlood", "Not Type O\n0.60",
  "Type O\n0.40", "Rh+\n0.975",
  "Type O\n0.40", "Rh-\n0.025",
  "Rh+\n0.975", "Type O+\n0.39",
  "Rh-\n0.025", "Type O-\n0.01"
)

ggflowchart(FC,
            horizontal = TRUE,
            arrow_size = 0.25,
            text_size = 4,
            x_nudge = 0.1)
```

We could repeat this process for types A, B, and AB, and make a giant chart with all the possibilities.


### Sampling from a population

The probabilities in the blood type table above represent what are known as "long-run" probabilities, meaning that if we sample more and more people (up to the entire population size), the proportions that we observe will more closely match the expected probabilities.

What happens if we sample from a small group, one that is much smaller than the entire population?

Questions like this become very important when thinking about blood donation programs. Not everyone donates blood, and the specific sample from which blood is sampled can have a dramatic effect on the relative supply of different blood types in a community.

```{webr-r}
#| context: setup

BT <- data.frame(
  Type = c("A-", "A+", "AB-", "AB+", "B-", "B+", "O-", "O+"),
  Probability = c(0.005, 0.270, 0.001, 0.070, 0.004, 0.250, 0.010, 0.390)
)

blood_sample <- function(people = 100) {
  DD <- tibble(Type = BT$Type,
               Count = rmultinom(n = 1,
                                 size = people,
                                 prob = BT$Probability))
  ggplot() +
    geom_col(data = tibble(Type = BT$Type,
                           Predicted = BT$Probability * people),
             aes(x = Type, y = Predicted)) +
    geom_point(data = DD,
               aes(x = Type, y = Count),
               color = "red", size = 4) +
    theme_light()
}

```

The function below mimics the process of blood donations from a random sample of `people`. Random samples (i.e., donors) are drawn from a distribution where the probability of each of the eight blood types occurring is the same as in the table above.

The gray bars are the expected counts, and the red points are the observed counts in a particular sample. Run the code a few times and examine how the plot changes each time.

```{webr-r}
blood_sample(people = 100)
```

> What do you observe about the distribution of counts in the observed sample (red points) compared to the predicted probabilities (gray bars)? What does this tell you about the need to specifically recruit donors with rare blood types?

<textarea rows="3" cols="75">

</textarea>

Go back to the code block above. Try increasing the number of people sampled and see how the distribution changes (once again, avoid very very large numbers)?

> What looks different when 1000 or 10000 people are sampled?

<textarea rows="3" cols="75">

</textarea>


## Goodness of fit

At this point you are gaining some intuition for how the random processes that are a normal part of sampling can lead to quite variable results each time a new sample is taken. Sometimes a sample will closely match the predicted probabilities:

- Occasionally 5 *D* alleles and 5 *d* alleles are present when 10 gametes are produced
- Occasionally the counts of blood types in a sample of 100 people agree well with the predictions

In both cases, as we increase the sample size (500, 1000, 10000, etc.), the observed counts more closely match the predicted counts.

It would be good to have a way to explicitly test whether the observed counts in a sample are a close match to what we expect. Essentially we want to know:

1. Do these observations represent a random sample from a given population?
2. Do these counts differ significantly from the expected counts for a given population?

These questions are basically asking the same thing. Both are answered by a statistical test for counts: a "goodness of fit" test. The name is fairly self-explanatory -- we are testing how "good" the observations "fit" the expected.

There are many different goodness of fit tests in statistics, but the one that is used most often in genetics is the **chi-squared test**^[Also called the chi-square test or the $\chi^2$ test]. That is the test we will use here.

Although the chi-squared test is fairly straightforward as we will see, the equation is a little daunting at first:

$$\chi^2 = \sum_{i=1}^n \frac{(Observed_i - Expected_i)^2}{Expected_i}$$

We can decode this "statistical sentence" piece by piece.

1. $\chi^2$ is the value that we are going to calculate. It is a single number that represents the test statistic. Somewhat confusingly, this number is not, itself, squared. Think of "chi-squared" as just the number calculated on the right side of the equation.
2. $\Sigma$ is the symbol for a summation, adding a set of numbers together
    - $i = 1$ tells us where to start adding
    - $n$ tells us where to stop adding. In this case $n$ represents the number of groups we are testing.
3. $(Observed_i - Expected_i)^2$ says to subtract the expected count from the observed count and then square that number, which is itself divided by the expect count ($Expected_i$). The subscript $i$s are related to the $i = 1$ to $n$ for the summation, just accounting for the fact that we'll do this calculation once for each group.

You can consider the equation as an efficient way to tell you what to do with all the counts you have. In plain language:

- For each group:
    - Subtract the expected count from the observed count, square it, and divide by the expected count
- Add up all those numbers to get the $\chi^2$ value.


### Using a chi-squared test

Before we get to the case study, we will take slight detour and apply the chi-squared test to a new data set. The data below are the counts of births on each day of the week for 350 consecutive births in a hospital (not all the births on each day happened on the same day -- that would be one very busy hospital).

```{webr-r}
DOB <- tribble(
  ~ Day, ~ Births,
  "Sunday", 33,
  "Monday", 41,
  "Tuesday", 63,
  "Wednesday", 63,
  "Thursday", 47,
  "Friday", 56,
  "Saturday", 47) |> 
  mutate(Day = ordered(Day, levels = Day))

DOB
```

Execute the code to save the data into R. If you are interested in what the code does:

- `tribble()` is a function that lets us enter data line by line. Here we are making columns for `Day` and `Births`. Each row represents one pair. Sunday has 33 births. Monday has 41, and so on.
- `mutate()` just puts the days in the correct order.

We can plot these data with the code block below.

```{webr-r}
ggplot(DOB, aes(x = Day, y = Births)) +
  geom_col(fill = "goldenrod") +
  theme_light()
```

For these data, the chi-squared test asks whether the number of births per day is equal across all the days. Since we expect that births happen randomly with respect to day of week (i.e., babies should have no concept of what day it is and should thus not prefer to be born on one day vs. another), this seems like a reasonable null hypothesis.

> Looking at the plot above, do you predict that births are evenly distributed (null hypothesis) or unevenly distributed?

<textarea rows="3" cols="75">

</textarea>

> If births are not evenly distributed throughout the week, what do you think might explain this pattern? What reason(s) might lead to more births on some days vs. others?

<textarea rows="3" cols="75">

</textarea>

We will find out.

Thinking back to the equation for the chi-squared test, let's figure out what we need to calculate. We have the observed counts (33, 41, 63, etc.). We first need to figure out the expect count of births per day. See if you can reason through what number that is.

> What is the expected count of births per day?

<textarea rows="3" cols="75">

</textarea>

We can also use R to do the calculation:

```{webr-r}
Expected <- sum(DOB$Births) / 7
Expected
```

Here are using `sum()` to add up the `Births` column of the `DOB` data. With 350 births spread over 7 days, we expect 50 per day. Now that we have the expected value, we can calculate $\chi^2$.

```{webr-r}
DOB <- DOB |> 
  mutate(Obs_Exp = Births - Expected,
         Obs_Exp_Sq = Obs_Exp^2)
DOB
```

We have done several things all at once in this code block. `mutate()` makes new columns. First we make a column `Obs_Exp` which is the observed count of `Births` minus `Expected` (50). We then use this new column to calculate the square of `Obs_Exp`.

Looking at the printout of the data, we can see that some days have more births than expected (Tuesday, Wednesday, Friday) and some have less (Sunday, Monday, Thursday, Saturday). All the squares are positive, as we expect (the squaring here is the derivation of the name $\chi^2$).

All that remains is to divide the `Obs_Exp2` by `Expected`:

```{webr-r}
DOB <- DOB |> 
  mutate(Obs_Exp_Sq_Std = Obs_Exp_Sq / Expected)
DOB
```

This last step normalizes the squared deviation by the expected count. You can imagine if there are a lot of observations, then the squared deviations can get quite large.

Finally, we add up the values in the last column. These values represent each day's contribution to the overall $\chi^2$ value.

```{webr-r}
X2 <- sum(DOB$Obs_Exp_Sq_Std)
X2
```

We have a value of 15.24. What do we do with it? Just on it's own, the $\chi^2$ statistic doesn't mean anything. We need a value to compare it to.

The way that the chi-squared test is set up, the test value (15.24) is compared to the value that marks the cutoff for some specified percentage of a chi-squared distribution.

The figure below shows the probability plot for a chi-squared distribution with 6 *degrees of freedom*. The concept of degrees of freedom in statistics is often confusing. One way to think about it is the number of values that can vary independently. For example, there are 10 total in two groups. If you know that one group has 6, then you also know that the other group has 4. So for two groups, there is just 1 degree of freedom (if you know one, then you can calculate the other).

There are 6 degrees of freedom in this example, because we have 7 groups (days). For this test degrees of freedom is $n - 1$. Like the sets of blood types probabilities, this also sums to 1. But instead of adding up individual probabilities, here we calculate (integrate) the area under the curve. The area under the line is 1.

The area shaded in purple represents 5% of the area under the line. Most commonly in biological sciences, 5% is used as the cutoff when deciding that a particular test is "significant" or not.^[For those interested, 5% is known as the alpha-level for the test. The choice of 5% is arbitrary but generally agreed on, for better or worse. There is a large literature on *P*-values, their use, and their misuse, but we don't have time to dig into that today.]

```{r}
#| echo: false

shade_chisq <- function(p, df) {
  crit <- qchisq(p, df, lower.tail = FALSE)
  M <- data.frame(x = seq(0.1, crit * 1.5, length = 200))
  M$y <- dchisq(M$x, df)
  p <- ggplot(M, aes(x, y)) +
    geom_line() +
    geom_ribbon(data = subset(M, x > crit),
                aes(ymax = y), ymin = 0,
                fill = "purple", alpha = 0.5) +
    ylim(c(0, 1.1 * max(M$y))) +
    labs(x = "Chi-squared Statistic", y = "Relative Probability")
  return(p)
}

shade_chisq(0.05, df = 6)

```

If the observed data were really drawn from a uniform distribution, meaning that counts for each day are equal, then 5% of the time, the $\chi^2$ statistic will fall somewhere in the purple area.

For our test, all we need to know is the position on the x-axis of the left-hand edge of the purple area. This value is about 12.6 (you can confirm this because the left edge is just past the thin vertical line at 12.5). 12.6 is known as the *critical value* for the test.

If the $\chi^2$ statistic is greater than the critical value, then we are able to reject the null hypothesis. Which says that these observations do not follow the expected distribution.

The test statistic we calculated was 15.24, which is greater than 12.6. So here we reject the null hypothesis. It appears that births do not happen randomly throughout the week.

### Shortcut

R has a built-in function to carry out the calculations for us. Although it is very informative to work through the calculations manually, it's much easier to just let the function do all the work for us. That function is `chisq.test()`.

```{webr-r}
chisq.test(DOB$Births)
```

The output shows much the same information that we already calculated: the $\chi^2$ statistic of 15.24 and 6 degrees of freedom. One new value is the exact *P*-value. All that we knew above was that *P* was less than 0.05. Now we know that it is exactly 0.01847.

If we plot the chi-squared distribution again, we can add the observed statistic. *P* = 0.01847 means that 1.847% of the area under the line falls to the right of the black dot.

```{r}
#| echo: false

shade_chisq(0.05, df = 6) +
  geom_point(x = 15.24, y = 0, size = 5)

```

Take a few minutes to explore how the sample size impacts the $\chi^2$ statistic and *P*-value. We can easily scale the data up and down: 

```{webr-r}
chisq.test(DOB$Births * 10)
```

```{webr-r}
chisq.test(DOB$Births / 10)
```

::: {.callout-note}

Note that if you scale down too far (e.g., `DOB$Births / 100`), you will receive a warning: `Chi-squared approximation may be incorrect`. This warning occurs because the chi-square test is not applicable for very low sample sizes. This warning is just telling you to be careful with the results in this case.

:::


> Do the results make sense to you based on what you know about how sample size impacts our certainty when making inferences about a sample?

<textarea rows="3" cols="75">

</textarea>


## References

::: {#refs}
:::
